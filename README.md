# CSCI-596
This final project was inspired from a portion of a current research project I'm working on. The big picture idea is to understand the nature of dark matter. To do this, we use analyze strong gravitational lensing of distant quasars. A quasar is a type of active galactic nuclei (AGN) located at the center of a galaxy which emits a lot of high energy photons. As these photons travel out into the universe and pass by nearby massive objects (such as galaxies), the laws of General Relativity tell us their path begins to curve around the massive object. The photons then create an image in a different location from their original emitted path known as a lensed image. This curved light path is affected by all massive objects in a galaxy. This includes both luminous matter and dark matter. Additionally, different types of dark matter models will produce slightly different gravitationally lensed images. Therefore strong gravitational lensing around galaxies can be used as a tracer to quantify the amount and type of dark matter in galaxies.

Since the dark matter is not yet known to interact with the electromagnetic force, dark matter is not visible to us (hence the name "dark" matter). This means that we are unable to observe the type of dark matter directly. So we need an alternative approach to directly observing dark matter. The general analysis technique is to perform simulations of galaxy evolution using a software called Galacticus and to compare simulated lensed galaxy images to actual observations. Dark matter is distributed throughout the galaxy in multiple spherical structures called subhalos, and subhalos collectively form a subhalo population. So specifically, Galacticus models how a subhalo population within a galaxy evolves over time, and different dark matter models can be input into the simulation (i.e. a galaxy composed of cold dark matter or a galaxy composed of warm dark matter can be modeled).

We don't just want to use Galacticus to generate a single subhalo population however. Instead, we want to simulate many different subhalo populations (previous studies have sampled ~1,200,000). The reason for this is that there are certain constraints that must be satisfied to analyze observational images. One example is that the lensed image positions from a generated Galacticus galaxy must match the observed lensed image positions, and only a subset of simulatedgalaxies will produce such lensed images. Not one single simulation will exactly match what's observed in each parameter, so only the generated subhalo populations which have the smallest discrepancy in parameter space get compared to the observational data.

So we want to generate many different subhalo populations, but unfortunately this is not computationally feasable. In order to generate a subhalo population, you first must Monte Carlo (MC) sample individual subhalos in the subhalo population. There may be around 500,000 subhalos in a given subhalo population. From there, each subhalo in the population will evolve forwards in time, and that process is modeled in Galacticus by numerically solving a differential equation. So the reason why generating many subhalo populations is not computationally feasible is because it requires numerically solving ~10^11-10^12 independent differential equations. This is where my part of the research project comes in, where we attempt to use a machine learning algorithm called a Normalizing Flows algorithm to speed this process up. 

We will first describe how a normalizing flows algorithm works, and then describe how we use the algorithm in our project. Like any machine learning algorithm, a normalizing flows algorithm both has an input and an output. The input to the algorithm is an arbitrary distribution of data points that the emulator is trying to replicate. The algorithm begins running not with the input distribution, but rather a second, simplified distribution such as a Gaussian distribution. When we say "simplified", we mean some distribution which does not take much time computationally to sample from. The emulator then begins to warp the Gaussian distribution so that it looks like the input distribution over a series of invertible transformations (the number of specific transformations is specified by the user). After the set number of transformations has been applied, each of the transformations can be strung together using function composition in order to create a single, invertible map going from the Gaussian space to the input distribution space. This map goes in both directions since it's invertible. This map also happens to be the output of the normalizing flows algorithm. Specifically, the user returns the weights from the individual invertible functions.

To see how the normalizing flows algorithm gets applied in our project, when we generate a population of subhalos, we are inherently generating a population of data points in a subhalo parameter space. The subhalo parameter space is a 6 dimensional parameter space, consisting of a subhalo's infall mass, bound mass, infall redshift, concentration, orbital radius and truncation radius. When we generate many different points in this parameter space, we are essentially sampling from the underlying distribution in this parameter space. This subhalo space acts as the input distribution in the normalizing flows algorithm, and the algorithm ends up learning a map from the Gaussian space to the subhalo space. The reason this is important is because rather than generating many different subhalo populations using Galacticus, we instead generate a single subhalo population in Galacticus, then use it as an input into the algorithm, then sample data points in the Gaussian space and apply the learned function from the algorithm to those data points to generate data points in the subhalo space. So while it takes the same amount of time to generate the first subhalo population and then it takes a while for the normalizing flows algorithm to learn th map from the Gaussian space to the subhalo space, every subhalo population after these first two steps is generated simply by applying a function to a set of Gaussian data, which is much quicker than before. 

We used a normalizing flows algorithm to speed up the process of running Galacticus many times. Galacticus is an example of a semi-analytic model (SAM). SAM's are a midpoint between analytically approximating a system and performing N-body simulations of a system. While N-body simulations provide accurate results of the input physics, they are computationally expensive to run. This is the same issue we ran into with Galacticus. So using the normalizing flows algorithm to generate several data sets of N-body simulation parameters is an immediate extension of how this work could be generalized. Additionally, this code works for a single type of dark matter (such as cold dark matter or warm dark matter). But often times we care about being more specific by explicitly stating the dark matter particle mass within the model. So going forwards, we would like to be able to create a model flexible enough to include the warm dark matter particle mass in the code to generate realizations of many different dark matter models simultaneously without having to run the algorithm for each dark matter model.
